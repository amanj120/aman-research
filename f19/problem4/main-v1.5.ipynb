{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 4: Document clustering\n",
    "\n",
    "_Version 1.5_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Suppose we have several documents and we want to _cluster_ them, meaning we wish to divide them into groups based on how \"similar\" the documents are. One question is what does it mean for two documents to be similar?\n",
    "\n",
    "In this problem, you will implement a simple method for calculating similarity. You are given a dataset where each document is an excerpt from a classic English-language book. Your task will consist of the following steps:\n",
    "\n",
    "1. Cleaning the documents\n",
    "2. Converting the documents into \"feature vectors\" in a data model\n",
    "3. Comparing different documents by measuring the similarity between feature vectors\n",
    "\n",
    "With that as background, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 0. Data cleaning\n",
    "\n",
    "Recall that the dataset is a collection of book excerpts. Run the next three cells below to see what the raw data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from problem_utils import read_files\n",
    "\n",
    "books, excerpts = read_files(\"data/\")\n",
    "print(f\"{len(books)} books found: {books}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here's an excerpt from one of the books, namely, [George Orwell's classic novel 1984](https://en.wikipedia.org/wiki/Nineteen_Eighty-Four)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{len(excerpts)} excerpts (type: {type(excerpts)})\")\n",
    "\n",
    "excerpt_1984 = excerpts[books.index('1984')]\n",
    "print(f\"\\n=== Excerpt from the book, '1984' ===\\n{excerpt_1984}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Normalizing Text\n",
    "\n",
    "As with any text analysis problem we will need to clean up this data. Start by cleaning the text as follows:\n",
    "\n",
    "* Convert all the letters to lowercase\n",
    "* Retain only alphabetic and space-like characters in the text.\n",
    "\n",
    "For example, the sentence,\n",
    "```python \n",
    "    '''How many more minutes till I get to 22nd and D'or street?'''\n",
    "``` \n",
    "\n",
    "becomes,\n",
    "```python\n",
    "    '''how many more minutes till i get to nd and dor street'''\n",
    "```\n",
    "\n",
    "**Exercise 0.a** (1 point). Create a function `clean_text(text)` which takes as input an \"unclean\" string, `text`, and returns a \"clean\" string per the specifications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    assert (isinstance(text, str)), \"clean_text expects a string as input\"\n",
    "    ### BEGIN SOLUTION\n",
    "    clean_text = text.lower()\n",
    "    clean_text = ''.join([c for c in clean_text if c.isalpha() or c.isspace()])\n",
    "    return clean_text\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_clean_text",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `test_clean_text` (1 point)\n",
    "\n",
    "# A few test cases:\n",
    "print(\"Running fixed tests...\")\n",
    "sen1 = \"How many more minutes till I get to 22nd and, D'or street?\"\n",
    "ans1 = \"how many more minutes till i get to nd and dor street\"\n",
    "\n",
    "assert (isinstance(clean_text(sen1), str)), \"Incorrect type of output. clean_text should return string.\"\n",
    "assert (clean_text(sen1) == ans1), \"Text incorrectly normalised. Output looks like '{}'\".format(clean_text(sen1))\n",
    "\n",
    "sen2 = \"This is\\n a whitespace\\t\\t test with 8 words.\"\n",
    "ans2 = \"this is\\n a whitespace\\t\\t test with  words\"\n",
    "assert (clean_text(sen2) == ans2), \"Text incorrectly normalised. Output looks like '{}'\".format(clean_text(sen2))\n",
    "print(\"==> So far, so good.\")\n",
    "\n",
    "# Some random instances;\n",
    "def check_clean_text_random(max_runs=100):\n",
    "    from random import randrange, random, choice\n",
    "    def rand_run(options, max_run=5, min_run=1):\n",
    "        return ''.join([choice(options) for _ in range(randrange(min_run, max_run))])\n",
    "    printable = [chr(k) for k in range(33, 128) if k is not ord('\\r')]\n",
    "    alpha_lower = [c for c in printable if c.isalpha() and c.islower()]\n",
    "    alpha_upper = [c.upper() for c in alpha_lower]\n",
    "    non_alpha = [c for c in printable if not c.isalpha()]\n",
    "    spaces = [' ', '\\t', '\\n']\n",
    "    s_in = ''\n",
    "    s_ans = ''\n",
    "    for _ in range(randrange(0, max_runs)):\n",
    "        p = random()\n",
    "        if p <= 0.5:\n",
    "            fragment = rand_run(alpha_lower)\n",
    "            fragment_ans = fragment\n",
    "        elif p <= 0.75:\n",
    "            fragment = rand_run(alpha_upper)\n",
    "            fragment_ans = fragment.lower()\n",
    "        elif p <= 0.9:\n",
    "            fragment = rand_run(non_alpha)\n",
    "            fragment_ans = ''\n",
    "        else:\n",
    "            fragment = rand_run(spaces, max_run=3)\n",
    "            fragment_ans = fragment\n",
    "        s_in += fragment\n",
    "        s_ans += fragment_ans\n",
    "    print(f\"\\n* Input: {s_in}\")\n",
    "    s_you = clean_text(s_in)\n",
    "    assert s_you == s_ans, f\"ERROR: Your output is incorrect: '{s_you}'.\"\n",
    "\n",
    "print(\"\\nRunning battery of random tests...\")\n",
    "for _ in range(20):\n",
    "    check_clean_text_random()\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's clean some excerpts!  \n",
    "\n",
    "**Exercise 0.b** (1 point). Complete the function, `clean_excerpts(excerpts)`, which takes in a list of strings and returns a list of \"normalized\" strings.\n",
    "\n",
    "> Note: `clean_excerpts` should return a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_excerpts(excerpts):\n",
    "    assert isinstance(excerpts, list), \"clean_excerpts expects a list of strings as input\"\n",
    "    ### BEGIN SOLUTION\n",
    "    clean_excerpts = [clean_text(e) for e in excerpts]\n",
    "    return clean_excerpts\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Run the following cells to clean our collection of excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "docs = clean_excerpts(excerpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_clean_excerpts",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_clean_excerpts` (1 point)\n",
    "\n",
    "docs = clean_excerpts(excerpts)\n",
    "\n",
    "puncts = ['‘', '…', '’', '—', ',', '”', '1', '“', '9', '5', '=', '?', '3', '!', ';', '\"', '(', '-', ':', ')', '_', '0', '7', '.', \"'\"]\n",
    "assert (isinstance(docs, list)), \"Incorrect type of output. clean_excerpts should return a list of strings.\"\n",
    "assert (len(docs) == len(excerpts)), \"Incorrect number of cleaned excerpts returned.\"\n",
    "\n",
    "for doc in docs:\n",
    "    for c in doc:\n",
    "        if c in puncts:\n",
    "            assert False, \"{} found in cleaned documents\".format(c)\n",
    "            \n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 1. Bag-of-Words\n",
    "\n",
    "To calculate similarity between two documents, a well-known technique is the _bag-of-words_ model. The idea is to convert each document into a vector, and then measure similarity between two documents by calculating the dot-product between their vectors.\n",
    "\n",
    "Here is how the procedure works. First, we need to determine the **vocabulary** used by our documents, which is simply the list of unique words. For instance, suppose we have the following two documents:\n",
    "* `doc1 = \"create ten different different sample\"`\n",
    "* `doc2 = \"create ten another another example example example\"`\n",
    "\n",
    "Then the vocabulary is\n",
    "\n",
    "* `['another', 'create', 'different', 'example', 'sample', 'ten']`\n",
    "\n",
    "Next, let's create a **feature vector** for each document. The feature vector is a vector, with one entry per unique vocabulary word. The value of each entry is the number of times the word occurs in the document. For example, the feature vectors for our two sample documents would be:\n",
    "\n",
    "```python \n",
    "vocabulary = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "doc1_features =  [0, 1, 2, 0, 1, 1]\n",
    "doc2_features = [2, 1, 0, 3, 0, 1]\n",
    "```\n",
    "\n",
    "> _Aside_: For a deeper dive into the bag-of-words model, refer to this [Wikipedia article](https://en.wikipedia.org/wiki/Bag-of-words_model). However, for this problem, what you see above is the gist of what you need to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "Not all words carry useful information for the purpose of a given analysis. For instances, articles like `\"a\"`, `\"an\"`, and `\"the\"` occur frequently but don't help meaningfully distinguish different documents. Therefore, we might want to omit them from our vocabulary.\n",
    "\n",
    "Suppose we have decided that we have determined the list, `stop_words`, defined below, to be such a Python set of stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stop_words = {'a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any',\n",
    "              'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did',\n",
    "              'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have',\n",
    "              'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its',\n",
    "              'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather',\n",
    "              'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their',\n",
    "              'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants',\n",
    "              'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will',\n",
    "              'with', 'would', 'yet', 'you', 'your'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Excercise 1.a** (1 point) Complete the function `extract_words(doc)`, below. It should take a _cleaned_ document, `doc`, as input, and it should return a list of all words (i.e., it should return a list of strings) subject to the following two conditions:\n",
    "\n",
    "1. It should omit any stop words, i.e., it should return only \"informative\" words.\n",
    "2. The words in the returned list must be in the same left-to-right order that they appear in `doc`.\n",
    "3. The function must return all words, even if they are duplicates.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "    # Omit stop words!\n",
    "    extract_words(\"what is going to happen to me\") == ['going', 'happen']\n",
    "    \n",
    "    # Return all words in-order, preserving duplicates:\n",
    "    extract_words(\"create ten another another example example example\") \\\n",
    "        == ['create', 'ten', 'another', 'another', 'example', 'example', 'example']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(doc):\n",
    "    assert isinstance(doc, str), \"extract_words expects a string as input\"\n",
    "    ### BEGIN SOLUTION\n",
    "    words = doc.split() \n",
    "    words_cleaned = [w for w in words if w not in stop_words]\n",
    "    return words_cleaned\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_extract_words",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_extract_words` (1 point)\n",
    "\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "\n",
    "sen1 = doc1\n",
    "ans1 = ['create', 'ten', 'different', 'different', 'sample']\n",
    "assert(isinstance(extract_words(sen1),list)), \"Incorrect type of output. extract_words should return a list of strings.\"\n",
    "assert(extract_words(sen1) == ans1), \"extract_words failed on {}\".format(sen1)\n",
    "\n",
    "sen2 = \"what is going to happen to me\"\n",
    "ans2 = ['going', 'happen']\n",
    "assert(extract_words(sen2) == ans2), \"extract_words failed on {}\".format(sen2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.b** (1 point). Next, let's create a vocabulary for the book-excerpt dataset.\n",
    "\n",
    "Complete the function, `create_vocab(list_of_documents)`, below. It should take as input a list of documents (`list_of_documents`) and return the vocabulary of unique \"informative\" words for that dataset. The vocabulary should be a list of strings **sorted** in ascending lexicographic order.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "create_vocab(doc_list) == ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "```\n",
    "\n",
    "> **Note 0.** We do not want any stop words in the vocabulary. Make use of `extract_words()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(list_of_documents):\n",
    "    assert isinstance(list_of_documents, list), \"create_vocab expects a list as input.\"\n",
    "    ### BEGIN SOLUTION\n",
    "    words = set()\n",
    "    for each_doc in list_of_documents:\n",
    "        w = set(extract_words(each_doc))\n",
    "        words |= w\n",
    "    return sorted(list(words))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_create_vocab",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_create_vocab` (1 point)\n",
    "\n",
    "doc1 = doc_list\n",
    "ans1 = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "assert(isinstance(create_vocab(doc1),list)), \"Incorrect type of output. create_vocab should return a list of strings.\"\n",
    "assert(create_vocab(doc1) == ans1), \"create_vocab failed on {}\".format(doc1)\n",
    "\n",
    "doc2 = [docs[books.index('gatsby')]]\n",
    "ans2 = ['abnormal', 'abortive', 'accused', 'admission', 'advantages', 'advice', 'afraid', 'again', 'always', 'anyone', 'appears', 'attach', 'attention', 'autumn', 'away', 'back', 'being', 'birth', 'boasting', 'book', 'bores', 'came', 'care', 'certain', 'closed', 'college', 'come', 'communicative', 'conduct', 'confidences', 'consequence', 'creative', 'criticizing', 'curious', 'deal', 'decencies', 'detect', 'didnt', 'dignified', 'dont', 'dreams', 'dust', 'earthquakes', 'east', 'elations', 'end', 'everything', 'excursions', 'exempt', 'express', 'extraordinary', 'father', 'feel', 'feigned', 'felt', 'few', 'find', 'flabby', 'floated', 'forever', 'forget', 'foul', 'found', 'founded', 'frequently', 'fundamental', 'gatsby', 'gave', 'gestures', 'gift', 'gives', 'glimpses', 'gorgeous', 'great', 'griefs', 'habit', 'hard', 'havent', 'heart', 'heightened', 'hope', 'horizon', 'hostile', 'human', 'im', 'impressionability', 'inclined', 'infinite', 'interest', 'intimate', 'intricate', 'itself', 'ive', 'judgements', 'last', 'levity', 'life', 'limit', 'little', 'machines', 'made', 'man', 'many', 'marred', 'marshes', 'matter', 'meant', 'men', 'miles', 'mind', 'missing', 'moral', 'more', 'name', 'natures', 'never', 'normal', 'nothing', 'obvious', 'one', 'opened', 'out', 'over', 'parceled', 'people', 'person', 'personality', 'plagiaristic', 'point', 'politician', 'preoccupation', 'preyed', 'privileged', 'privy', 'promises', 'quality', 'quick', 'quivering', 'reaction', 'readiness', 'realized', 'register', 'related', 'remember', 'repeat', 'represented', 'reserve', 'reserved', 'reserving', 'responsiveness', 'revelation', 'revelations', 'right', 'riotous', 'rock', 'romantic', 'scorn', 'secret', 'sense', 'sensitivity', 'series', 'shall', 'shortwinded', 'sign', 'sleep', 'snobbishly', 'something', 'sorrows', 'sort', 'still', 'successful', 'such', 'suggested', 'suppressions', 'temperament', 'temporarily', 'ten', 'terms', 'those', 'thousand', 'told', 'tolerance', 'turned', 'turning', 'unaffected', 'unbroken', 'under', 'understood', 'unequally', 'uniform', 'unjustly', 'unknown', 'unmistakable', 'unsought', 'unusually', 'up', 'usually', 'veteran', 'victim', 'vulnerable', 'wake', 'wanted', 'way', 'wet', 'weve', 'whenever', 'wild', 'world', 'years', 'young', 'younger', 'youve']\n",
    "assert(create_vocab(doc2) == ans2), \"create_vocab failed on {}\".format(doc2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.c** (2 points). Given a list of documents and a vocabulary, let's create bag-of-words vectors for each document.\n",
    "\n",
    "Complete the function `bagofwords(doclist, vocab)`, below. It takes as input a list of documents (`doclist`) and a list of vocabulary words (`vocab`). It will return a list of bag-of-words vectors, with one vector for each document in the input.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "vocab = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "bagofwords(doc_list, vocab) == [[0, 1, 2, 0, 1, 1],\n",
    "                                [2, 1, 0, 3, 0, 1]]\n",
    "```\n",
    "\n",
    "> **Note 0**: Every word in the document must be present in the vocabulary. Therefore you should use the same preprocessing function (`extract_words()`) that was used to create the vocabulary.\n",
    ">\n",
    "> **Note 1**: `bagofwords()` should return a list of vectors, where each vector is a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagofwords(doclist, vocab):\n",
    "    assert (isinstance(doclist, list)), \"bagofwords expects a list of strings as input for doclist.\"\n",
    "    assert (isinstance(vocab, list)), \"bagofwords expects a list of strings as input for vocab.\"\n",
    "    ### BEGIN SOLUTION\n",
    "    bow = []\n",
    "    for doc in doclist:\n",
    "        doc_words = extract_words(doc)\n",
    "        bag = [0]*(len(vocab))\n",
    "        for w in doc_words:\n",
    "            i = vocab.index(w)\n",
    "            bag[i] += 1\n",
    "        bow.append(bag)\n",
    "    return bow\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_bagofwords_1",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_bagofwords_1` (1 point)\n",
    "\n",
    "doc1 = doc_list\n",
    "vocab1 = create_vocab(doc1)\n",
    "vec1 = [0, 1, 2, 0, 1, 1]\n",
    "assert(isinstance(bagofwords(doc1, vocab1),list)), \"Incorrect type of output. bagofwords should return a list of integers.\"\n",
    "assert(bagofwords(doc1, vocab1)[0] == vec1), \"bagofwords failed on {}\".format(doc1)\n",
    "\n",
    "doc2 = [docs[books.index('1984')][-200:]]\n",
    "vocab2 = create_vocab(doc2)\n",
    "vec2 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert(bagofwords(doc2, vocab2)[0] == vec2), \"bagofwords failed on {}\".format(doc2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_bagofwords_2",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_bagofwords_2` (1 point)\n",
    "\n",
    "print(\"\"\"\n",
    "This test cell will be replaced with one hidden test case.\n",
    "You will only know the result after submitting to the autograder.\n",
    "If the autograder times out, then either your solution is highly\n",
    "inefficient or contains a bug (e.g., an infinite loop).\n",
    "\"\"\")\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "word_counts = {'kiterunner': 142, 'littlewomen': 212, 'janeeyre': 126, 'gatsby': 212,\n",
    "               'prideandprejudice': 335, '1984': 401,'olivertwist': 415, 'littleprince': 270,\n",
    "               'prisonerofazkaban': 537, 'hamlet': 519}\n",
    "literature_vocab = create_vocab(docs)\n",
    "bow = bagofwords(docs, literature_vocab)\n",
    "\n",
    "for i in range(len(bow)):\n",
    "    nwords = len(bow[i]) - bow[i].count(0)\n",
    "    assert (nwords == word_counts[books[i]]), \"Number of words returned for {} is incorrect\".format(books[i])\n",
    "    \n",
    "print(\"\\n (Passed)\")\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let us take a look at the number of words found in our BoW vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(books)):\n",
    "    bow = bagofwords(docs, create_vocab(docs))\n",
    "    print('{:17s}\\t: {} words'.format(books[i],len(bow[i])-bow[i].count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Normalization (Again?)\n",
    "One of the artifacts you might have noticed from the BoW vectors is that they have very different number of words. This is because the excerpts are of different lengths which may artificially skew the norms of these vectors.  \n",
    "\n",
    "One way to remove this bias is to keep the direction of the vector but normalize the lengths to be equal to one. If the vector is $\\mathbf{v} = \\begin{bmatrix} v_0\\\\ v_1\\\\ \\vdots\\\\ v_{n-1}\\end{bmatrix} \\in \\mathbf{R}^n$, then its unit-normalized version is $\\mathbf{\\hat{v}}$, given by\n",
    "  \n",
    "$$\n",
    "\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\lVert \\mathbf{v}\\rVert_2} = \\frac{\\mathbf{v}}{\\sqrt{v_0^2 + v_1^2 + \\ldots + v_{n-1}^2}}.\n",
    "$$\n",
    "\n",
    "For instance, recall the BoW vectors from our earlier example:\n",
    "```python\n",
    "bow = [[0, 1, 2, 0, 1, 1],\n",
    "       [2, 1, 0, 3, 0, 1]]\n",
    "```\n",
    "\n",
    "The normalized versions would be\n",
    "```python\n",
    "bow_normalize = [[0.0, 0.3779644730092272, 0.7559289460184544, 0.0, 0.3779644730092272, 0.3779644730092272],\n",
    "                 [0.5163977794943222, 0.2581988897471611, 0.0, 0.7745966692414834, 0.0, 0.2581988897471611]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.d** (2 points). Complete the function `bow_normalize(bow)`, below. It should take as input a list of BoW vectors. It should return their unit-normalized versions, per the formula above, also as a **list of vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_normalize(bow):\n",
    "    assert(isinstance(bow,list)),\"bow_normalize expects a list of ints as input\"\n",
    "    ### BEGIN SOLUTION\n",
    "    norm_bow = []\n",
    "    for v in bow:\n",
    "        normv = math.sqrt(sum([vi**2 for vi in v]))\n",
    "        unitv = [vi/normv for vi in v]\n",
    "        norm_bow.append(unitv)\n",
    "    return norm_bow\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_normalize",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "bow0 = [[1, 2, 3, 1, 1], [2, 2, 2, 2, 0]]\n",
    "nbow0 = [[0.25, 0.5, 0.75, 0.25, 0.25], [0.5, 0.5, 0.5, 0.5, 0]]\n",
    "\n",
    "ans0 = bow_normalize(bow0)\n",
    "assert(isinstance(ans0,list)), \"Incorrect type of output. bow_normalize should return a list of floats.\"\n",
    "\n",
    "assert(len(nbow0[0]) == len(ans0[0])), \"bow_normalize failed on {}\".format(bow0[0])\n",
    "for i, el in enumerate(nbow0[0]):\n",
    "    assert (math.isclose(ans0[0][i], el,rel_tol=1e-6,abs_tol=1e-8)), \"bow_normalize failed on element {}\".format(el)\n",
    "assert(len(nbow0[1]) == len(ans0[1])), \"bow_normalize failed on {}\".format(bow0[1])\n",
    "for i, el in enumerate(nbow0[1]):\n",
    "    assert (math.isclose(ans0[1][i], el,rel_tol=1e-6,abs_tol=1e-8)), \"bow_normalize failed on element {}\".format(el)\n",
    "\n",
    "bow1 = [[0, 1, 2, 0, 1, 1],\n",
    "       [2, 1, 0, 3, 0, 1]]\n",
    "nbow1 = [[0.0, 0.3779644730092272, 0.7559289460184544, 0.0, 0.3779644730092272, 0.3779644730092272],\n",
    "    [0.5163977794943222, 0.2581988897471611, 0.0, 0.7745966692414834, 0.0, 0.2581988897471611]]\n",
    "\n",
    "ans1 = bow_normalize(bow1)\n",
    "assert(len(nbow1[0]) == len(ans1[0])), \"bow_normalize failed on {}\".format(bow1[0])\n",
    "for i, el in enumerate(nbow1[0]):\n",
    "    assert (math.isclose(ans1[0][i], el,rel_tol=1e-6,abs_tol=1e-8)), \"bow_normalize failed on element {}\".format(el)\n",
    "assert(len(nbow1[1]) == len(ans1[1])), \"bow_normalize failed on {}\".format(bow1[1])\n",
    "for i, el in enumerate(nbow1[1]):\n",
    "    assert (math.isclose(ans1[1][i], el,rel_tol=1e-6,abs_tol=1e-8)), \"bow_normalize failed on element {}\".format(el)\n",
    "\n",
    "print(\"==> So far, so good.\")\n",
    "# Some Random Instances\n",
    "def check_bow_normalize_random():\n",
    "    from random import choice, sample\n",
    "    \n",
    "    vec_len =  choice(range(2,8))\n",
    "    nvecs = choice(range(3,7))\n",
    "    rvecs = []\n",
    "    for _ in range(nvecs):\n",
    "        rvecs.append(sample(range(2*vec_len),vec_len))\n",
    "        \n",
    "    unit_rvecs = bow_normalize(rvecs)\n",
    "    \n",
    "    for i in range(len(unit_rvecs)):\n",
    "        print(\"Input {}\".format(rvecs[i]))\n",
    "        u = unit_rvecs[i]\n",
    "        ans = 1.0;\n",
    "        \n",
    "        for ui in u:\n",
    "            ans = ans - (ui*ui)\n",
    "            \n",
    "        assert (math.isclose(ans,0,rel_tol=1e-6,abs_tol=1e-8)), \"ERROR: Your output is incorrect {}\".format(u)\n",
    "        \n",
    "print(\"\\nRunning battery of random tests...\")\n",
    "for _ in range(20):\n",
    "    check_bow_normalize_random()\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "(_Aside_) **Sparsity of BoW vectors.** As an aside, run the next cell to see the BoW vectors are actually quite _sparse_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "literature_vocab = create_vocab(docs)\n",
    "bow = bagofwords(docs, literature_vocab)\n",
    "nbow = bow_normalize(bow)\n",
    "numterms = len(docs)*len(literature_vocab)\n",
    "numzeros = sum([b.count(0) for b in nbow])\n",
    "print(\"Percentage of entries which are zero: {:.1f} %\".format(100*numzeros/numterms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "> If everything is correct, you'll see that the BoW vectors are sparse, with about 85-86% of the components being zeroes. Therefore, we could in principle save a lot of space by only storing the non-zeroes. While we do not exploit this fact in our current example it is useful to think about these costs while running analytics at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 2. Comparing Documents\n",
    "\n",
    "Now we have normalized vector versions of each document, we can use a standard similarity measure to compare vectors. For this question, we shall use the *inner product*. Recall that the inner product of two vectors $a,b \\in \\mathbf{R}^n$ is defined as,\n",
    "\n",
    "$$<a,b> = \\Sigma_{i=0}^{n-1} a_i b_i$$\n",
    "\n",
    "For example,\n",
    "\n",
    "$$<[1,-1,3], [2,4,-1]> = (1\\times2)+(-1\\times4)+(3\\times-1)=-5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2.a** (1 point) Complete the function `inner_product(a, b)` which takes two vectors, `a` and `b`, both represented as lists, and returns their inner product.\n",
    "\n",
    "> Note: `inner_product(a, b)` should return a value of type `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner_product(a,b):\n",
    "    assert (isinstance(a, list)), \"inner_product expects a list of floats/ints as input for a.\"\n",
    "    assert (isinstance(b, list)), \"inner_product expects a list of floats/ints as input for b.\"\n",
    "    assert len(a) == len(b), \"inner_product should be called on vectors of the same length.\"\n",
    "    ### BEGIN SOLUTION\n",
    "    prod = sum([ia*ib for ia,ib in zip(a,b)])\n",
    "    return float(prod)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_inner_product",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_inner_product` (0.5 point)\n",
    "\n",
    "vec1a = [1,-1,3]\n",
    "vec1b = [2,4,-1]\n",
    "ans1 = -5\n",
    "assert (isinstance(inner_product(vec1a,vec1b),float)), \"Incorrect type of output. inner_product should return a float.\"\n",
    "assert (inner_product(vec1a,vec1b) == ans1), \"inner_product failed on inputs {} and {}\".format(vec1a,vec1b)\n",
    "assert (inner_product(vec1b,vec1a) == ans1), \"inner_product failed on inputs {} and {}\".format(vec1b,vec1a)\n",
    "\n",
    "vec2a = [0,2,1,9,-1]\n",
    "vec2b = [17,4,1,-1,0]\n",
    "ans2 = 0\n",
    "assert (inner_product(vec2a,vec2b) == ans2), \"inner_product failed on inputs {} and {}\".format(vec2a,vec2b)\n",
    "assert (inner_product(vec2b,vec2a) == ans2), \"inner_product failed on inputs {} and {}\".format(vec2b,vec2a)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We can use the `inner_product()` as a measure of similarity between documents! (_Recall the linear algebra refresher in Topic 3_.) In particular, since our normalized BoW vectors are \"direction\" vectors, the inner product measures how closely two vectors point in the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.b** (1 point). Now we can finally answer our initial question: which book excerpts are similar to each other? Complete the function `most_similar(nbows, target)`, below, to answer this question. In particular, it should take as input the normalized BoW vectors created in the previous part, as well as a target excerpt index $i$. It should return most index of the excerpt most similar to $i$.\n",
    "\n",
    "> **Note 0.** Ties in scores are won by the smaller index. For example, if excerpt 2 and excerpt 7 both equally similar to the target excerpt 8, then return 2 as the most similar excerpt.\n",
    ">\n",
    "> **Note 1.** Your `most_similar()` function should return a value of type `int`.\n",
    "\n",
    "> **Note 2.** The test cell refers to hidden tests, but in fact, the test is not hidden per se. Instead, we are hashing the strings returned by your solution to be able to check your answer without revealing it to you directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(nbows, target):\n",
    "    assert (isinstance(nbows,list)), \"most_similar expects list as input for nbows.\"\n",
    "    assert (isinstance(target,int)), \"most_similar expects integer as input for target.\"\n",
    "    ### BEGIN SOLUTION\n",
    "    most_sim_idx = -1\n",
    "    most_sim_val = -1\n",
    "    \n",
    "    # For the first half (j<i)\n",
    "    for j in range(len(nbows)):\n",
    "        if j == target:\n",
    "            continue # Don't check similarity to self\n",
    "        val = inner_product(nbows[j],nbows[target])\n",
    "        if (val > most_sim_val):\n",
    "            most_sim_idx = j\n",
    "            most_sim_val = val\n",
    "            \n",
    "    return most_sim_idx\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test_most_similar",
     "locked": true,
     "points": "1",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Cell: `test_most_similar` (1 point)\n",
    "literature_vocab = create_vocab(docs)\n",
    "bow = bagofwords(docs, literature_vocab)\n",
    "nbow = bow_normalize(bow)\n",
    "\n",
    "# Start with two basic cases:\n",
    "target1 = books.index('1984') \n",
    "ans1 = books.index('kiterunner')\n",
    "assert (isinstance(most_similar(nbow,target1),int)), \"most_similar should return integer.\"\n",
    "assert (most_similar(nbow,target1) == ans1), \"most_similar failed on input {}\".format(books[target1])\n",
    "\n",
    "target2 = books.index('prideandprejudice')\n",
    "ans2 = books.index('hamlet')\n",
    "assert (most_similar(nbow,target2) == ans2), \"most_similar failed on input {}\".format(books[target2])\n",
    "\n",
    "# Check the rest via obscured, hashed solutions\n",
    "### BEGIN HIDDEN TESTS\n",
    "def most_similar__soln(nbows, target):\n",
    "    assert (isinstance(nbows,list)), \"most_similar expects list as input for nbows.\"\n",
    "    assert (isinstance(target,int)), \"most_similar expects integer as input for target.\"\n",
    "    most_sim_idx = -1\n",
    "    most_sim_val = -1    \n",
    "    for j in range(len(nbows)): # For the first half (j<i)\n",
    "        if j == target:\n",
    "            continue # Don't check similarity to self\n",
    "        val = inner_product(nbows[j],nbows[target])\n",
    "        if (val > most_sim_val):\n",
    "            most_sim_idx = j\n",
    "            most_sim_val = val            \n",
    "    return most_sim_idx\n",
    "\n",
    "# Generate and store some reference solutions\n",
    "def gen_most_similar_solns(filename='most_similar_solns.csv'):\n",
    "    from os.path import isfile\n",
    "    from problem_utils import make_hash\n",
    "    if not isfile(filename):\n",
    "        with open(filename, 'wt') as fp_soln:\n",
    "            for i in range(len(books)):\n",
    "                j = most_similar__soln(nbow, i)\n",
    "                soln_hashed = make_hash(books[j])\n",
    "                fp_soln.write(f'{books[i]},{soln_hashed}\\n')\n",
    "                \n",
    "gen_most_similar_solns()\n",
    "### END HIDDEN TESTS\n",
    "def check_most_similar_solns():\n",
    "    from problem_utils import make_hash, open_file\n",
    "    literature_vocab = create_vocab(docs)\n",
    "    bow = bagofwords(docs, literature_vocab)\n",
    "    nbow = bow_normalize(bow)\n",
    "    with open_file(\"most_similar_solns.csv\", \"rt\") as fp_soln:\n",
    "        for line in fp_soln.readlines():\n",
    "            target_name, soln_hashed = line.strip().split(',')\n",
    "            target_id = books.index(target_name)\n",
    "            your_most_sim_id = most_similar(nbow, target_id)\n",
    "            assert isinstance(your_most_sim_id, int), f\"Your function returns a value of type {type(your_most_sim_id)}, not an integer\"\n",
    "            assert 0 <= your_most_sim_id < len(nbow), f\"You returned {your_most_sim_id}, which is an invalid value (it should be between 0 and {nbow})\"\n",
    "            your_most_sim_name = books[your_most_sim_id]\n",
    "            print(f\"For book '{target_name}', you calculated '{your_most_sim_name}' as most similar.\")\n",
    "            your_most_sim_name_hashed = make_hash(your_most_sim_name)\n",
    "            assert your_most_sim_name_hashed == soln_hashed, \"==> ERROR: Unfortunately, your returned value does not appear to match our reference solution.\"\n",
    "\n",
    "check_most_similar_solns()\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now let's have a look at the documents most similar to each other, according to your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(len(books)):\n",
    "    jdx = most_similar(nbow,idx)\n",
    "    print(books[idx],\"is most similar to\",books[jdx],\"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** You’ve reached the end of this part. Don’t forget to restart and run all cells again to make sure it’s all working when run in sequence; and make sure your work passes the submission process. Good luck!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
