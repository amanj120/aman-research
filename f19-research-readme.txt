A Summary of Fall 2019 Research - Aman Jain

Over the last four months, I have done a variety of work in natural language processing and linear algebra. All of the relevant code I have written this semester can be found on https://github.com/amanj120/f19-research.git. I will be mentioning files and notebooks from this repository frequently through the remainder of this document as I try to explain how I spent my time, what I learned, and the milestones I reached.

I had a few personal goals coming into this semester regarding research, and I accomplished most of them to a varying degree. The biggest goal I had was to gain some insight into the research process and find a topic that engaged me. I enjoy the work I've done/will continue to do because it satisfies my curiosity about math and it's applications. I also really like that we are working on projects from the ground up, not just using a bunch of different libraries to accomplish some specific task. I think that the work I am doing is very applicable to a variety of topics.

With that being said, I can now explain what I've done. The first six (ish) weeks were spent quickly developing a background in Tensor/Linear Algebra based NLP. The remainder of the semester was spent applying the concepts I learned to try and run a clustering algorithm on a set of documents I extracted from an email newsletter I am subscribed to. This newsletter, called "The New Paper", is a daily email each containing 4-7 condensed summaries of major events. Each of these summaries was about 40 words or three sentences.

The first set of instructional material I went through was in the folder titled 'Notebook 15 - SVD and PCA'. The first jupyter notebook inside, called 'part0.ipynb', was about using the SVD of a data matrix to conduct a principal component analysis. The second jupyter notebook, 'part1.ipynb' is about using the SVD to compress an image. To fully understand these notebooks, I had to refresh myself on the SVD and some other core linear algebra concepts. These notebooks helped me grasp how linear algebra could be used to extract latent data from large datasets, and how it could be used to condense large datasets into compact and essential information.

The next thing I went through was the material in the folder 'problem4'. This material was about constructing a "Bag Of Words" model for a set of documents and using the bag of words to find the similarity between two given documents. The notebooks also went through a lot of basics about how to normalize textual data and how to construct mathematical objects representing text information. A lot of the ideas in these notebooks would be replicated in my work. 

In the 'papers' folder is a research paper by Bader and Kolda titled 'Tensor Decompositions and Applications'. This paper was very important to the work I did because it outlined two different methods of Tensor Decompositions, how to implement them, and their applications. One of the methods called CP Decomposition would be the main method I would be using moving forward.

The folder titled 'problem7-blogs' was about a handwritten implementation of MTTKRP (an integral component of conducting a CP Decomposition). The notebook was complicated and I don't understand a lot of it, but that didn't end up being too much of a problem because I would go on to use prebuilt libraries to conduct my actual tensor decompositions once I started needing to do them.

After getting a decent background in this material, I moved on to writing some of my own code.

The first notebook I worked on is called 'Word Vector.ipynb'. This notebook tries to implement word vectors in a way that allows them to do addition and subtraction (I was trying to replicate the framework which yields word vector algebra such as King - Man + Woman = Queen). The corpus I used was about thirty words, which was obviously too small to give any meaningful results. It was a good learning experience though, as I got to implement some sort of scheme that converted words into vectors, and I was able to do some math on those vectors to translate them back into words. For example, using the word vectors I was able to find that 'georgia' , 'public', 'research', and 'university' were all similar words to 'clemson', which makes a lot of sense.

The next notebook I worked on was called 'Tensor Construction POC.ipynb'. This notebook simply served to explain how I made the larger tensor in the actual New Paper folder and what properties it followed. Once again, this notebook was a good learning experience in normalization and construction of sparse tensors. 

The last preliminary notebooks I worked on is called 'NLTK Tests.ipynb'. This notebook was actually created after I started the New Paper project. I was new to NLTK and I wanted to test out the Part Of Speech Tagging feature on NLTK. The goal of this was top hopefully be able to use the POS tagging to consolidate proper nouns into single tokens (for example, the phrase 'Prime Minister Boris Johnson' would be treated as a single token 'prime_minister_boris_johnson' instead of four separate tokens). I never ended up using this feature in the actual New Paper work though simply because I didn't think it would change the results I got.

Now we can move to the folder titled 'New_Paper'.

The fundamental idea behind this project was to take the ~1000 documents I had extracted from the emails I got, and group them based on topic similarity. The way I would go about doing this was to first create a 3-dimensional tensor representing the documents and the word pairs in the document. After doing this, I would run a Rank-N CP Decomposition on that tensor. From this decomposition, one of the resulting values would be a matrix with size N * (number of documents). Each of the N rows on this matrix would then ideally be a clustering of documents based on some hidden attributes of the original tensor, and the actual values of the rows would indicate how much a single document "belonged" in that clustering (i.e. if documents a,b, and c on a given clustering on row i had values of 20, 100, and 1 respectively then we know that document b is probably the most representative document in that clustering, and document a is probably closer to b than document c is). This was my understanding, but as far as I know, I could be wrong.

To try and accomplish this goal I first had to take the emails from my inbox and convert them into .txt files. This is precisely what the jupyter notebook titled 'New Paper Mbox to txt.ipynb' does, along with a little bit of preprocessing such as getting rid of UTF-8 punctuation that didn't translate to ASCII, removing hyperlinks, and lowercasing everything. These processed documents are stored in the folder called 'parsed_docs'. 

After converting the .mbox (mailbox) file to text documents, I had to actually construct the tensor. This is what the notebook 'docs to tensor.ipynb' does. Using a similar method to the one in 'Tensor Construction POC.ipynb', I construct a word-word-document tensor and export it as a set of .csv files called 'indexes.csv' (which contained the indexes of the non-zero values in the tensor) and 'values.csv' (which contained the values corresponding to the indexes in 'indexes.csv'). Both of these files can be found in the folder called 'csvs'. I had to export the tensor in such a fashion so that I could reconstruct the sparse tensor in MATLAB. 

Here are descriptions of some other files in the 'csvs' folder that were created up until this point.

'sentence_lengths.csv' - a document with the length of sentences. From this document I was able to find some information about what our tensor should look like: The average sentence length was ~11.7 words, and each document had on average 3 sentences, so there were, on average, 3 * 12^2/2 ~ 216 pairs. Furthermore, 97% of pairs showed up only once in each document, thus, ln(p(x,y,z)/p(x)p(y)) -- I omitted p(z) because it was constant -- is approximately ln((1/216)/(5/36000)(5/36000)) ~ 12.4. In practice, we see that our average PMI value was 10.4 with a standard deviation of 2.2. Also, our values are normally distributed, so we should expect ~68% of our data to fall within 1 standard deviation, and 67.3% of our actual data does fall within the range [8.2,12.6].

'counts.csv' keeps track of the number of times a word shows up in our document. This showed me that on average, a distinct word shows up ~5 times in the entire corpus, which is really not a lot.

'words.txt' is a dictionary of the words in our corpus in the format 'word' : (index of the word, amount of times that word shows up in our corpus). 

'freqs.csv' - honestly, I have no idea what this is. I think it might be a version of 'counts.csv' from a different tensor construction. I can't find where it's generated anywhere. 

After the tensor construction, I had to run a CP Decomposition on the tensor. 

One of the stumbling points for me was with a library called TensorD. This library was supposed to be a python equivalent of Tensor Toolbox. I tried running the example scripts they outlined in their documentation in a notebook called 'tensorD demo.ipynb', but I ran into a lot of errors. I decided after a certain point that continuing to work on tensorD would be a poor use of my time, and I switched to MatLab and tensor toolbox, which is much more robust and has more features anyways. 

There are two .m files in the New_Paper directory. The first one, 'check_cp_als.m' was a program I wrote to try out tensor toolbox and MatLab since both of these tools were completely new to me. The second file, 'tensor_decomp_matlab_test.m' was the actual program I used to read in 'indexes.csv' and 'values.csv', construct a sparse tensor object, and conduct a CP Decomposition on it. I did a rank 100 decomposition on the tensor, achieved about a 5% fit to the original tensor, and exported the Document-Rank matrix (size was 100 * 1043 [1043 being the number of documents]) as a csv called 'fibers.csv'. I also exported the lambda values (a 100 *1 vector) just in case I wanted to do further calculations about the strength of one group od clusterings compared to another.

After obtaining these two csv files from MatLab, I wrote a brief notebook called 'Translating Tensors.ipynb' which took the 'fibers.csv' file and actually extracted the clusters from it. For now, the only thing I did was check to see which indexes had a value NOT in the range [-1,1]. I figured anything outside of that range meant something significant. After implementing this simple filter, I exported those clusters in a file called 'lens_and_clusters.csv'. 

Another type of clustering I did was a more basic version that didn't require MATLAB. After constructing the tensor, I went through each individual document matrix, and for each of those, I found the five other matrices most closely resembling the original. This analysis was exported to a file called 'subtraction_clusters.csv'. This type of clustering is just a variation of a 'bag of words' document clustering, so it's no surprise that it does a good job of pairing up documents that use similar phrases or words. For example, an article about a Capital One data breach was very similar to a document about an Equifax data breach, since both documents had phrases like 'data breach', 'exposed personal information', and 'social security numbers'. 

The CP Decomposition yielded some strange clusterings. I haven't looked much into it yet, but on a surface level, it doesn't make much sense. For example, one of the clusters (Cluster #3) clustered together documents about SpaceX's Falcon rockets, E-cigarettes, Nobel Laureates in Literature, and US-China trade (documents 656, 979, 937, and 646 respectively).

Moving forward, I think the single biggest thing that can be done to provide a more accurate method of analyzing document tensors is finding a decomposition that is much more accurate. Somewhere above 75% would be ideal. The other two papers in that "papers" folder outline a method of CP Decomposition for sparse tensors, and that is something that I am going to try and implement in the future.

Once a more accurate decomposition is found, another thing that would be important to do is more thoroughly analyze what the clusters mean. For example, in my current implementation, I haven't paid any attention to the idea of "weighted clusters" as I mentioned earlier. 

In short, I really feel like any forward progress in this project is dependent on finding a more accurate decomposition. This is where the paper titled ‘ Provable Sparse Tensor Decomposition’ comes into play. A method is outlined in that paper, called Tensor Truncated Power which sounds promising. I will try to implement that in python and see if I get accurate results, and hopefully, that will work.

That is a summary of everything I’ve done this semester, the things I learned, the things I accomplished, and the things I want to do moving forward.